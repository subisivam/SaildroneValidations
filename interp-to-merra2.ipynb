{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce csv\n",
    "# import packages\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## export a SD file with ... ##\n",
    "# time, lon, lat, QL, QS, qs, qa, SST, T, V\n",
    "sd0 = pd.read_csv('data/sd/avg-at30-2017.csv')\n",
    "\n",
    "p = sd0['p'] # hPa\n",
    "T = sd0['T']+273.15 # C to K\n",
    "SST = sd0['SST']+273.15 # C to K\n",
    "RH = sd0['RH'] # %\n",
    "\n",
    "# conversions for sea\n",
    "# sea saturation vapor pressure : Wallace and Hobbs, Second Edition (pg. 99)\n",
    "es = 6.11*np.exp((2.50*10**6*18.016/(1000*8.3145))*((1/273)-(1/SST))) #hPa\n",
    "e = es * (RH/100.0) # hPa\n",
    "qs = 0.622*(e/(p-e))*1000 #g/kg\n",
    "\n",
    "# conversions for air\n",
    "es = 6.11*np.exp((2.50*10**6*18.016/(1000*8.3145))*((1/273)-(1/T))) # air saturation vapor pressure : Wallace and Hobbs, Second Edition (pg. 99)\n",
    "e = es * (RH/100.0) # vapor pressure : Wallace and Hobbs, Second Edition (pg. 82)\n",
    "qa = 0.622*(e/(p-e))*1000 # specific humidity of atmosphere : Wallace and Hobbs (pg. 80)\n",
    "\n",
    "t = sd0['datetime']\n",
    "lon = sd0['longitude']\n",
    "lat = sd0['latitude']\n",
    "QL = sd0['QL']\n",
    "QS = sd0['QS']\n",
    "V = sd0['V']\n",
    "\n",
    "sd = np.array([t,lon,lat, QL,qs,qa,QS,SST,T,V,p])\n",
    "sd = pd.DataFrame(sd.T,columns=['datetime','lon','lat','QL','qs','qa','QS','SST','T','V','p'])\n",
    "sd.to_csv('data/sd/sd-for-m2-2017.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove land points\n",
    "# import packages\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sd = pd.read_csv('data/m2/sd-for-m2-2019.csv')\n",
    "\n",
    "t = sd['datetime']\n",
    "lon = sd['lon']\n",
    "lat = sd['lat']\n",
    "QL = sd['QL']\n",
    "QS = sd['QS']\n",
    "qs = sd['qs']\n",
    "qa = sd['qa']\n",
    "SST = sd['SST']\n",
    "T = sd['T']\n",
    "V = sd['V']\n",
    "\n",
    "land = pd.read_csv('check-land/m2land.csv')\n",
    "land_lat = land['lat']\n",
    "land_lon = land['lng']\n",
    "land = [land_lon,land_lat]\n",
    "land = np.array(land)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the rest\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on the Earth's surface\n",
    "    using the Haversine formula.\n",
    "    \n",
    "    lon1, lat1: Longitude and latitude of the first point (in degrees).\n",
    "    lon2, lat2: Longitude and latitude of the second point (in degrees).\n",
    "    \n",
    "    Returns the distance in kilometers.\n",
    "    \"\"\"\n",
    "    # Convert degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(math.radians, [lon1, lat1, lon2, lat2])\n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = 6371 * c  # Earth's radius in kilometers\n",
    "    return distance\n",
    "def find_nearest(coordinates, target_point):\n",
    "    closest_index = 0\n",
    "    closest_distance = haversine(target_point[0], target_point[1], coordinates[0][0], coordinates[0][1])\n",
    "    for i, coordinate in enumerate(coordinates):\n",
    "        distance = haversine(target_point[0], target_point[1], coordinate[0], coordinate[1])\n",
    "        if distance < closest_distance:\n",
    "            closest_index = i\n",
    "            closest_distance = distance\n",
    "    return closest_index\n",
    "def hours_to_unix_timestamp(hours_array):\n",
    "    # Set the starting date as May 1, 2019\n",
    "    start_date = np.datetime64('2019-05-01T00:30:00')\n",
    "    # Calculate the time difference in seconds for the entire array\n",
    "    time_difference_seconds = (hours_array * 3600)\n",
    "    # Convert the time differences to timedelta64 objects\n",
    "    time_difference_timedelta = time_difference_seconds.astype('timedelta64[s]')\n",
    "    # Add the time difference to the starting date\n",
    "    result_dates = start_date + time_difference_timedelta\n",
    "    # Convert the result dates to Unix timestamps\n",
    "    unix_timestamps = (result_dates - np.datetime64('1970-01-01')) / np.timedelta64(1, 's')\n",
    "    return unix_timestamps\n",
    "# pretty ones from Eli\n",
    "def solution(X1: np.ndarray, X2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        X1: (1D array_like)\n",
    "        X2: (1D array_like)\n",
    "    Returns:\n",
    "        X1_indices where value exists in X2 as well\n",
    "        X2_indices where value exists in X1 as well\n",
    "    Note: the returned indices array are ordered smallest to greatest. by the value they correspond to\n",
    "    that is to say X1[X1_indices] is a sorted list, u could do X1[X1_indices.sort()] to get the values in \n",
    "    the order they appear in the orignal X1\n",
    "    \"\"\"\n",
    "    inter = np.intersect1d(X1, X2)\n",
    "    def helper(inter: np.ndarray, x: np.ndarray):\n",
    "        sorter = np.argsort(x)\n",
    "        searchsorted_left = np.searchsorted(x, inter, sorter=sorter,side='left')\n",
    "        searchsorted_right = np.searchsorted(x, inter, sorter=sorter,side='right')\n",
    "        values = vrange(searchsorted_left, searchsorted_right) \n",
    "        return sorter[values] # optional to sort this if u care?\n",
    "    return helper(inter, X1), helper(inter, X2)\n",
    "def vrange(starts: np.ndarray, stops: np.ndarray):\n",
    "    \"\"\"Create concatenated ranges of integers for multiple start/stop\n",
    "    Parameters:\n",
    "        starts (1-D array_like): starts for each range\n",
    "        stops (1-D array_like): stops for each range (same shape as starts)\n",
    "    Returns:\n",
    "        numpy.ndarray: concatenated ranges\n",
    "    For example:\n",
    "        >>> starts = [1, 3, 4, 6]\n",
    "        >>> stops  = [1, 5, 7, 6]\n",
    "        >>> vrange(starts, stops)\n",
    "        array([3, 4, 4, 5, 6])\n",
    "    \"\"\"\n",
    "    stops = np.asarray(stops)\n",
    "    l = stops - starts # Lengths of each range.\n",
    "    return np.repeat(stops - l.cumsum(), l) + np.arange(l.sum())\n",
    "\n",
    "prd = 'raw-data/m2/2019.nc'\n",
    "f = nc.Dataset(prd,mode='r')\n",
    "\n",
    "tprd = f.variables['time'][:] # hours since 05/01/2019 00:30:00\n",
    "lonprd = f.variables['lon'][:] # degrees\n",
    "latprd = f.variables['lat'][:] # degrees\n",
    "QLprd = f.variables['EFLUXWTR'][:] # W*m^-2\n",
    "qaprd = f.variables['QV10M'][:]*1000 # g/kg\n",
    "QSprd = f.variables['HFLUXWTR'][:] # W*m^-2\n",
    "SSTprd = f.variables['TSKINWTR'][:] # K\n",
    "Tprd = f.variables['T10M'][:] # K\n",
    "uprd = f.variables['U10M'][:] # m*s^-1\n",
    "vprd = f.variables['V10M'][:] # m*s^-1\n",
    "\n",
    "tprd = hours_to_unix_timestamp(tprd)\n",
    "Vprd = np.sqrt(uprd**2+vprd**2)\n",
    "\n",
    "# organize coordinates\n",
    "location = np.stack((lon,lat),axis=1) # set saildrone coordinates\n",
    "\n",
    "gridlocation = [] # set MERRA2 coordinates\n",
    "grididx = []\n",
    "for idx_lon, lon in enumerate(lonprd):\n",
    "    for idx_lat,lat in enumerate(latprd):\n",
    "        gridlocation.append([lon,lat])\n",
    "        grididx.append([idx_lon,idx_lat])\n",
    "gridlocation = np.asarray(gridlocation)\n",
    "grididx = np.asarray(grididx)\n",
    "\n",
    "lonidx = []\n",
    "latidx = []\n",
    "tttidx = []\n",
    "for idx,loc in enumerate(location):\n",
    "    k = find_nearest(gridlocation, loc)\n",
    "    # Initialize a flag to check if any match is found\n",
    "    match_found = False\n",
    "    for land_coord in land.T:\n",
    "        if gridlocation[k][0] == land_coord[0] and gridlocation[k][1] == land_coord[1]:\n",
    "            # Set the flag to True if a match is found\n",
    "            match_found = True\n",
    "            break\n",
    "    # Append indices only if no match is found\n",
    "    if not match_found:\n",
    "        lonidx.append(grididx[k, 0])\n",
    "        latidx.append(grididx[k, 1])\n",
    "        tttidx.append(idx)\n",
    "\n",
    "# organize times\n",
    "t = np.copy(t[tttidx])\n",
    "[idxprd,idxsd] = solution(tprd,t)\n",
    "idxprd = np.array(idxprd)\n",
    "tidxprd = np.copy(tprd[idxprd])\n",
    "QLidxprd = np.copy(QLprd[idxprd])\n",
    "QSidxprd = np.copy(QSprd[idxprd])\n",
    "Tidxprd = np.copy(Tprd[idxprd])\n",
    "SSTidxprd = np.copy(SSTprd[idxprd])\n",
    "qaidxprd = np.copy(qaprd[idxprd])\n",
    "Vidxprd = np.copy(Vprd[idxprd])\n",
    "\n",
    "# interpolate MERRA2 variables\n",
    "QL0 = []\n",
    "qa0 = []\n",
    "QS0 = []\n",
    "SST0 = []\n",
    "T0 = []\n",
    "SST0 = []\n",
    "V0 = []\n",
    "\n",
    "for tt in t:\n",
    "    for idx,ttprd in enumerate(tidxprd):\n",
    "        if tt == ttprd:\n",
    "            QL0.append(QLidxprd[idx])\n",
    "            qa0.append(qaidxprd[idx])\n",
    "            QS0.append(QSidxprd[idx])\n",
    "            SST0.append(SSTidxprd[idx])\n",
    "            T0.append(Tidxprd[idx])\n",
    "            V0.append(Vidxprd[idx])\n",
    "QL0 = np.array(QL0)\n",
    "qa0 = np.array(qa0)\n",
    "QS0 = np.array(QS0)\n",
    "SST0 = np.array(SST0)\n",
    "T0 = np.array(T0)\n",
    "V0 = np.array(V0)\n",
    "\n",
    "tidx = np.arange(0,len(t),1)\n",
    "QL = []\n",
    "qa = []\n",
    "QS = []\n",
    "SST = []\n",
    "T = []\n",
    "SST = []\n",
    "V = []\n",
    "\n",
    "for j in tidx:\n",
    "    # first closest\n",
    "    QL.append(QL0[j,latidx[j],lonidx[j]])\n",
    "    qa.append(qa0[j,latidx[j],lonidx[j]])\n",
    "    QS.append(QS0[j,latidx[j],lonidx[j]])\n",
    "    SST.append(SST0[j,latidx[j],lonidx[j]])\n",
    "    T.append(T0[j,latidx[j],lonidx[j]])\n",
    "    V.append(V0[j,latidx[j],lonidx[j]])\n",
    "\n",
    "latprd = np.array(latprd[latidx])\n",
    "lonprd = np.array(lonprd[lonidx])\n",
    "qa = np.array(qa)\n",
    "SST = np.array(SST)\n",
    "T = np.array(T)\n",
    "\n",
    "esT = 6.11*2.71828**(5420*(1/273-1/T)) # air saturation vapor pressure OK\n",
    "esSST = 6.11*2.71828**(5420*(1/273-1/SST)) # sea saturation vapor pressure OK\n",
    "\n",
    "rat_prh = 0.622*esT*((qa/1000+1)/(qa/1000)) #mb/rh dec\n",
    "qs = 1000*311*esSST/(500*rat_prh-311*esSST)\n",
    "\n",
    "lon = np.copy(sd['lon'][tttidx])\n",
    "lat = np.copy(sd['lat'][tttidx])\n",
    "prd = np.array([t,lon,lat,lonprd,latprd,QL,qs,qa,QS,SST,T,V])\n",
    "prd = prd.T\n",
    "prd = np.ma.masked_where(prd == 999999986991104, prd)\n",
    "prd = np.ma.compress_rows(prd)\n",
    "prd = np.ma.masked_where(prd == 1000000000000000, prd)\n",
    "prd = np.ma.compress_rows(prd)\n",
    "prd = pd.DataFrame(prd,columns=['datetime','lon','lat','lon-prd','lat-prd','QL','qs','qa','QS','SST','T','V'])\n",
    "prd.to_csv('data/m2/m2-2019.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arctic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
